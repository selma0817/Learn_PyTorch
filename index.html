<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-perceptrons" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/02/15/perceptrons/" class="article-date">
  <time class="dt-published" datetime="2024-02-16T03:33:28.788Z" itemprop="datePublished">2024-02-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h1><h2 id="Perceptron-Update"><a href="#Perceptron-Update" class="headerlink" title="Perceptron Update"></a>Perceptron Update</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">w_d &lt;- 0</span><br><span class="line">b &lt;- 0</span><br><span class="line">for iter = 1 ... maxIter</span><br><span class="line">	pred_y = np.dot(w_d, x_d) + b</span><br><span class="line">	if pred_y * y &lt; 0:</span><br><span class="line">	w_d &lt;- w_d + y * x_d</span><br><span class="line">	b &lt;- b + y</span><br><span class="line">return w_d, b</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>we update the weight and bias of the perceptron <strong>only when we made mistakes in prediction</strong> .  the y takes only value of 1 or -1. if our prediction is correct, then <code>sign(w_d * x_d) + b = sign(y)</code>. </p>
<h2 id="Perceptron-Convergence"><a href="#Perceptron-Convergence" class="headerlink" title="Perceptron Convergence"></a>Perceptron Convergence</h2><p>the perceptron only converge when </p>
<ol>
<li>there is hyperplane that perfectly separate all data in sample sapce </li>
<li>the radius of the data is bounded by a finite number</li>
</ol>
<p><strong>Theorem (Block &amp; Novikoff)</strong><br>Assuming input data has finite radius of $R$, meaning $|x^{(i)}|&lt;R, \forall x^{(i)}$<br>Assuming there exist unit hyperplane $\theta^{\star}$ that perfectly separate all data, meaning $y^{(i)}\theta^{\star}x^{(i)} \geq \gamma$, $||\theta^||&#x3D;1$</p>
<p>we conclude that the amount of  prediction mistakes we made is  is bounded by $(R&#x2F;\gamma)^2$. since we only update $\theta$ if we made mistakes, we will stop updating after we reach iteration $k &#x3D; (R&#x2F;\gamma)^2$. </p>
<p><strong>Proof</strong><br>given we are in $kth$ iteration and we made another mistake.<br>under linearly separable assumption, we have<br>$$<br>\theta^{k+1} &#x3D; \theta^{k} + (y^{(i)}x^{(i)})\<br>\theta^{k+1}  \theta^{\star} &#x3D; \theta^{k} \theta^{\star}+ (y^{(i)}\theta^{\star}x^{(i)})\<br>\theta^{k+1}  \theta^{\star} \geq \theta^{k} \theta^{\star} + \gamma \<br>\theta^{k+1} \theta^{\star} \geq k\gamma \<br>|\theta^{k+1}| |\theta^{\star}| &#x3D; |\theta^{k+1}| \geq \theta^{k+1} \theta^{\star} \geq k\gamma \<br>$$</p>
<p>under the radius of data is bounded assumption, we have<br>$$<br>|\theta^{k+1}|^{2} &#x3D; |\theta^{k}|^2 + 2(y^{(i)} \theta ^{k}x^{(i)})+|y^{(i)}x^{(i)}|^2<br>$$<br>since we only update when we made mistake<br>$$<br>y^{(i)} \theta ^{k}x^{(i)} &lt; 0<br>$$</p>
<p>$$<br>|\theta^{k+1}|^{2} \geq  | \theta^{k}|^2 |y^{(i)}x^{(i)}|^2<br>$$<br>$y^{(i)}$ only takes value of $1$ and $-1$<br>$$<br>|\theta^{k+1}|^{2} &#x3D;  | \theta^{k}|^2 |x^{(i)}|^2 \<br>|\theta^{k+1}|^{2} \leq  | \theta^{k}|^2 R^2 \<br>|\theta^{k+1}|^{2} \leq  k R^2 \<br>|\theta^{k+1}| \leq \sqrt{k} R<br>$$</p>
<p>combining the 2 inequalities above, we get<br>$$<br>k\gamma \leq |\theta^{k+1}| \leq \sqrt{k} R \<br>k \leq (R&#x2F;\gamma)^2 \<br>Q.E.D.<br>$$</p>
<h3 id="implications"><a href="#implications" class="headerlink" title="implications"></a>implications</h3><p>after proving that our perceptron algorithm converges after $(R&#x2F;\gamma)^2$ iteration given the input is bounded by radius $R$ and the input is linearly separable by unit hyperplane, we know that </p>
<ol>
<li>perceptron converges quickly when the margin is large , slowly when it is small</li>
<li>perceptron converges quickly when radius is small and slowly when it is large</li>
<li><strong>our bound does not depend on the number of inputs $N$. or the dimension of the input $d$.</strong></li>
<li>the proof guarantee perceptron converges, <strong>but there is no guarantee that it converges to the max margin hyperplane.</strong></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/02/15/perceptrons/" data-id="clso3wfml0000fzihbh9n8u4q" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-graycode" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/29/graycode/" class="article-date">
  <time class="dt-published" datetime="2023-12-29T08:58:22.363Z" itemprop="datePublished">2023-12-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/29/graycode/">LeetCode 89 Gray Code</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="LeetCode-89-Gray-Code"><a href="#LeetCode-89-Gray-Code" class="headerlink" title="LeetCode 89: Gray Code"></a>LeetCode 89: Gray Code</h2><p>Gray Code is <strong>a form of binary</strong> that use different method of incrementing from one number to next. in Gray Code, there is only 1 bit change from one number to the next. This format give us a way to do error checking: if there is more than 1 bit changed, something must be incorrect.<br>To convert from binary to Gray Code, we keep the most significant digit in binary and check for each digit, whether their previous digit is the same. If same, we put 0 in same index of gray code, else, we put 1. </p>
<table>
<thead>
<tr>
<th>Decimal</th>
<th>Binary</th>
<th>Gray Code</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>0000</td>
<td>0000</td>
</tr>
<tr>
<td>1</td>
<td>0001</td>
<td>0001</td>
</tr>
<tr>
<td>2</td>
<td>0010</td>
<td>0011</td>
</tr>
<tr>
<td>3</td>
<td>0011</td>
<td>0010</td>
</tr>
<tr>
<td>4</td>
<td>0100</td>
<td>0110</td>
</tr>
<tr>
<td>5</td>
<td>0101</td>
<td>0111</td>
</tr>
<tr>
<td>6</td>
<td>0110</td>
<td>0101</td>
</tr>
<tr>
<td>7</td>
<td>0111</td>
<td>0100</td>
</tr>
</tbody></table>
<p>we could also recover binary from gray code easily following reverse logic. To convert from decimal to Gray Code, we could left shift the integer once. for example if we want to convert integer <strong>n</strong>, we can do <strong>n&gt;&gt;1</strong> so that the left most digit in n is now 0. we then do <strong>XOR</strong> of n&gt;&gt;1 with n. so the formula is $$(n&gt;&gt;1 ) \oplus{ n}$$</p>
<p>To put it into code, if we want to generate n-bit gray code sequence, we can represent decimal number from 0 to (2^n-1)</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function">vector&lt;<span class="type">int</span>&gt; <span class="title">grayCode</span><span class="params">(<span class="type">int</span>  n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="type">int</span> length = <span class="built_in">pow</span>(<span class="number">2</span>,n);</span><br><span class="line">	vector&lt;<span class="type">int</span>&gt; result;</span><br><span class="line">	<span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;length; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		result.<span class="built_in">push_back</span>(i^(i&gt;&gt;<span class="number">1</span>));</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>





      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/29/graycode/" data-id="clqqeli210000kzihgjnt9d8i" data-title="LeetCode 89 Gray Code" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-subset II" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/29/subset%20II/" class="article-date">
  <time class="dt-published" datetime="2023-12-29T08:56:49.380Z" itemprop="datePublished">2023-12-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/29/subset%20II/">LeetCode 90 Subset II</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="LeetCode-90-SubSet-II"><a href="#LeetCode-90-SubSet-II" class="headerlink" title="LeetCode 90 SubSet II"></a>LeetCode 90 SubSet II</h1><h2 id="DFS-Backtracking"><a href="#DFS-Backtracking" class="headerlink" title="DFS, Backtracking"></a>DFS, Backtracking</h2><p>Given an array of numbers that contain duplicate, we are asked to generate all possible subsets(the power set). we do this by <strong>DFS</strong> and <strong>Backtracking</strong>. for each element of the initial array, we either include or not include it. to draw out our selection process in form of tree, at each number, we have 2 branches: include, or not include a number. the depth of the tree is n and the tree have <code>2^n</code> leave nodes. each leave node is a subset. </p>
<p>since we need to return a vector of vector of integer <code>vector&lt;vector&lt;int&gt;&gt;</code>, we create helper function called dfs and take in this vector of vector of integer, an integer vector, the original array, and the current number we iterate on as input. </p>
<p>what our recursion is doing here is essentially testing from all element from index 0 to index of num.size()-1. at each time we either add the current element to it and move to next recursion or don’t add the current element to it. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">Solution</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="type">void</span>  <span class="title">dfs</span><span class="params">(vector&lt;<span class="type">int</span>&gt; nums, vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; &amp;result, vector&lt;<span class="type">int</span>&gt; temp, <span class="type">int</span>  n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span>(n&gt;=nums.<span class="built_in">size</span>())</span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">if</span>(<span class="built_in">find</span>(result.<span class="built_in">begin</span>(), result.<span class="built_in">end</span>(), temp)==result.<span class="built_in">end</span>())</span><br><span class="line">	&#123;</span><br><span class="line">	result.<span class="built_in">push_back</span>(temp);</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// include num[n]</span></span><br><span class="line">temp.<span class="built_in">push_back</span>(nums[n]);</span><br><span class="line"><span class="built_in">dfs</span>(nums, result, temp, n+<span class="number">1</span>);</span><br><span class="line"><span class="comment">// exclude num[n]</span></span><br><span class="line">temp.<span class="built_in">pop_back</span>();</span><br><span class="line"><span class="built_in">dfs</span>(nums, result, temp, n+<span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">subsetsWithDup</span>(vector&lt;<span class="type">int</span>&gt;&amp;  nums) </span><br><span class="line">&#123;</span><br><span class="line">	<span class="built_in">sort</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">	vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; result;</span><br><span class="line">	vector&lt;<span class="type">int</span>&gt; temp;</span><br><span class="line">	<span class="built_in">dfs</span>(nums, result, temp, <span class="number">0</span>);</span><br><span class="line">	<span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>The logic behind sorting is that we don’t want duplicate. for now we are excluding duplicate by searching in result whether a particular candidate of subset already exist. but since we are looking for subset, the order does’t matter as long as the elements are all the same. but because we use vector of vector, the different ordered vector of same element will be viewed as different set. that is why we need to presort the array. </p>
<p>If we want to avoid sorting the array before hand, we could use set of vector of int <code>set&lt;vector&lt;int&gt;&gt;</code> and add to the set. Before we return we could loop through every element in set to put them into a vector. </p>
<p>Alternative recursive method is: </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">Solution</span> &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">	vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; <span class="built_in">subsetsWithDup</span>(vector&lt;<span class="type">int</span>&gt;&amp;  nums) </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">sort</span>(nums.<span class="built_in">begin</span>(), nums.<span class="built_in">end</span>());</span><br><span class="line">		vector&lt;vector&lt;<span class="type">int</span>&gt;&gt; result;</span><br><span class="line">		vector&lt;<span class="type">int</span>&gt; temp;</span><br><span class="line">		<span class="built_in">dfs</span>(nums, result, temp, <span class="number">0</span>);</span><br><span class="line">		<span class="keyword">return</span> result;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">	<span class="function"><span class="type">void</span>  <span class="title">dfs</span><span class="params">(vector&lt;<span class="type">int</span>&gt;&amp;  nums,vector&lt;vector&lt;<span class="type">int</span>&gt;&gt;&amp;  result, vector&lt;<span class="type">int</span>&gt;&amp;  temp, <span class="type">int</span>  start)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">	result.<span class="built_in">push_back</span>(temp);</span><br><span class="line">	<span class="keyword">for</span> (<span class="type">int</span> i = start; i &lt; nums.<span class="built_in">size</span>(); i++) </span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span> (i &gt; start &amp;&amp; nums[i] == nums[i - <span class="number">1</span>]) <span class="keyword">continue</span>;</span><br><span class="line">		temp.<span class="built_in">push_back</span>(nums[i]);</span><br><span class="line">		<span class="built_in">dfs</span>(nums, result, temp, i+<span class="number">1</span>);</span><br><span class="line">		temp.<span class="built_in">pop_back</span>();</span><br><span class="line">	&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>The difference between these two recursive method is how we expand the decision tree. in the first method, we loop through all element and consider each element individually on whether we add or not add. in the second method, each layer include all feasible choices of element to add. so as we go deeper in the tree, we have less and less option to add to subset. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/29/subset%20II/" data-id="clqqeja1r0000c4ih55n95xpy" data-title="LeetCode 90 Subset II" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-GoogLeNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/28/GoogLeNet/" class="article-date">
  <time class="dt-published" datetime="2023-12-28T09:05:10.097Z" itemprop="datePublished">2023-12-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/28/GoogLeNet/">GoogleNet Practice</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><p>the main advantage of googleNet is that is reduce the number of parameter needed by using inception blocks. Inception blocks use fewer parameter compare to using bigger convolution kernel. stacking inception blocks allow us to make network deeper </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">Inception1</span>(nn.Module):</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">__init__</span>(<span class="params">self, c1,c2,c3,c4,**kwargs</span>):</span><br><span class="line">		<span class="built_in">super</span>(Inception1, self).__init__(**kwargs)</span><br><span class="line">		<span class="comment"># Branch 1</span></span><br><span class="line">		self.b1_1 = nn.LazyConv2d(out_channels=c1, kernel_size=<span class="number">1</span>)</span><br><span class="line">		<span class="comment"># Branch 2</span></span><br><span class="line">		self.b2_1 = nn.LazyConv2d(out_channels=c2[<span class="number">0</span>], kernel_size=<span class="number">1</span>)</span><br><span class="line">		self.b2_2 = nn.LazyConv2d(out_channels=c2[<span class="number">1</span>], kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">		<span class="comment"># Branch 3</span></span><br><span class="line">		self.b3_1 = nn.LazyConv2d(out_channels=c3[<span class="number">0</span>], kernel_size = <span class="number">1</span>)</span><br><span class="line">		self.b3_2 = nn.LazyConv2d(out_channels=c3[<span class="number">1</span>], kernel_size = <span class="number">5</span>, padding = <span class="number">2</span>)</span><br><span class="line">		<span class="comment"># Branch 4</span></span><br><span class="line">		<span class="comment"># we need to specify the stride for max pool is 1 here</span></span><br><span class="line">		<span class="comment"># else it would get default to kernel_size to prevent overlapping</span></span><br><span class="line">		self.b4_1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">		self.b4_2 = nn.LazyConv2d(out_channels=c4, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">		b1 = F.relu(self.b1_1(x))</span><br><span class="line">		b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))</span><br><span class="line">		b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))</span><br><span class="line">		b4 = F.relu(self.b4_2(self.b4_1(x)))</span><br><span class="line">		<span class="keyword">return</span> torch.cat((b1, b2, b3, b4), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">GoogleNet1</span>(d2l.Classifier):</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">b1</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> nn.Sequential(</span><br><span class="line">		nn.LazyConv2d(<span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>), nn.ReLU(),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">b2</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> nn.Sequential(</span><br><span class="line">		nn.LazyConv2d(<span class="number">64</span>, kernel_size=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">		nn.LazyConv2d(<span class="number">192</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),nn.ReLU(),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">b3</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> nn.Sequential(</span><br><span class="line">		Inception1(<span class="number">64</span>,(<span class="number">96</span>,<span class="number">128</span>),(<span class="number">16</span>,<span class="number">32</span>), <span class="number">32</span>),</span><br><span class="line">		Inception1(<span class="number">128</span>, (<span class="number">128</span>,<span class="number">92</span>), (<span class="number">32</span>,<span class="number">96</span>), <span class="number">64</span>),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">b4</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> nn.Sequential(</span><br><span class="line">		Inception1(<span class="number">192</span>, (<span class="number">96</span>, <span class="number">208</span>), (<span class="number">16</span>, <span class="number">48</span>), <span class="number">64</span>),</span><br><span class="line">		Inception1(<span class="number">160</span>, (<span class="number">112</span>, <span class="number">224</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">		Inception1(<span class="number">128</span>, (<span class="number">128</span>, <span class="number">256</span>), (<span class="number">24</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">		Inception1(<span class="number">112</span>, (<span class="number">144</span>, <span class="number">288</span>), (<span class="number">32</span>, <span class="number">64</span>), <span class="number">64</span>),</span><br><span class="line">		Inception1(<span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">	)</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">b5</span>(<span class="params">self</span>):</span><br><span class="line">		<span class="keyword">return</span> nn.Sequential(</span><br><span class="line">		Inception1(<span class="number">256</span>, (<span class="number">160</span>, <span class="number">320</span>), (<span class="number">32</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">		Inception1(<span class="number">384</span>, (<span class="number">192</span>, <span class="number">384</span>), (<span class="number">48</span>, <span class="number">128</span>), <span class="number">128</span>),</span><br><span class="line">		nn.AdaptiveAvgPool2d((<span class="number">1</span>,<span class="number">1</span>)), nn.Flatten())	</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">__init__</span>(<span class="params">self, lr=<span class="number">0.1</span>, num_classes=<span class="number">10</span></span>):</span><br><span class="line">		<span class="built_in">super</span>(GoogleNet1, self).__init__()</span><br><span class="line">		self.save_hyperparameters()</span><br><span class="line">		self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),</span><br><span class="line">		self.b5(), nn.LazyLinear(num_classes))</span><br><span class="line">		self.net.apply(d2l.init_cnn)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = GoogleNet1().layer_summary((<span class="number">1</span>, <span class="number">1</span>, <span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = GoogleNet1(lr=<span class="number">0.01</span>)</span><br><span class="line">trainer = d2l.Trainer(max_epochs=<span class="number">10</span>, num_gpus=<span class="number">1</span>)</span><br><span class="line">data = d2l.FashionMNIST(batch_size=<span class="number">128</span>, resize=(<span class="number">96</span>, <span class="number">96</span>))</span><br><span class="line">model.apply_init([<span class="built_in">next</span>(<span class="built_in">iter</span>(data.get_dataloader(<span class="literal">True</span>)))[<span class="number">0</span>]], d2l.init_cnn)</span><br><span class="line">trainer.fit(model, data)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/28/GoogLeNet/" data-id="clqozcxcy0000b0ih5kgt33yy" data-title="GoogleNet Practice" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Why CNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/27/Why%20CNN/" class="article-date">
  <time class="dt-published" datetime="2023-12-27T09:45:09.741Z" itemprop="datePublished">2023-12-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/27/Why%20CNN/">Why switch to CNN from MLP</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Why-switch-to-Convolution-from-MLP"><a href="#Why-switch-to-Convolution-from-MLP" class="headerlink" title="Why switch to Convolution from MLP"></a>Why switch to Convolution from MLP</h2><p><strong>Regular Neural Net</strong> with many layers of fully connected neurons don’t scale well with full image. MLP has input layer, many hidden layers, and output layers. the neurons within a layer is independent of each other and fully connected to all neurons from its previous layer and next layer. <img src="https://cs231n.github.io/assets/nn1/neural_net2.jpeg" alt="MLP structure"> </p>
<p>We can see that MLP doesn’t scale well with full image since we need to have weights for all features. for 32 * 32 <em>3 image, we would have 32</em>32<em>3 &#x3D; 3072 weights in one layer. larger image size like 200</em>200<em>3 would lead to 200</em>200*3 &#x3D; 120,000 weights. with many hidden layers, this amount would increase more. <strong>full connectivity is wasteful and would lead to overfitting quickly</strong></p>
<h2 id="CNN-design"><a href="#CNN-design" class="headerlink" title="CNN design"></a>CNN design</h2><p>In CNN, neurons in a layer would only be connected to a small region of the layer before it. This region size is controlled by the kernel we use. the Idea behind it is that for image, each object in image should be more related to the adjacent pixels. hence we use a n*n kernel. Another reason is that the same object moving or rotating in the image shouldn’t change the label of it. So that is why the peripheral pixel is important and should be consistent for each labels. <img src="https://cs231n.github.io/assets/cnn/cnn.jpeg" alt="CNN structure"> </p>
<h3 id="Layers-of-CNN"><a href="#Layers-of-CNN" class="headerlink" title="Layers of CNN"></a>Layers of CNN</h3><p>there are 3 main layers for CNN. <strong>Convolutional layer</strong>, <strong>Pooling layer</strong>, and <strong>Fully-connected layer</strong>(same as in MLP).<br>The Convolutional layers contains parameters that can be trained: the value in kernel. the Pooling layer and activation layer don’t have parameters to train.<br><strong>Summary</strong></p>
<ol>
<li>For CNN, the input is 3D volume of image and the output is 3D volume of image</li>
<li>Conv layer has trainable parameters. Conv, Pooling, FC layer have hyper-parameters. activation layer don’t have any parameters. </li>
<li>the number of parameter for each layer is equal to input channel size * output channel size * kernel width * kernel height</li>
<li>we switch from fully connected to kernel view because for high dimensional input like images, it is infeasible (parameter explode) to train so many weights. so we capture the most valuable information through using kernel to extract a local region of the input volume.</li>
</ol>
<h3 id="Local-Connectivity"><a href="#Local-Connectivity" class="headerlink" title="Local Connectivity"></a>Local Connectivity</h3><p>convolutional kernel size is equivalent to <strong>receptive field</strong>. filter all have depth since the input is 3D. the depth is <strong>always equal to depth of input volume</strong>. for 32<em>32</em>3 image, and filter size of 5<em>5. the depth of the filter <strong>must</strong> be 3. so the filter is 5</em>5<em>3. the amount of parameter for a single filter is 5</em>5<em>3 &#x3D; 75 +1 bias &#x3D; 76 trainable parameters. if the input is 16</em>16<em>20 and we use 3</em>3 filter, the depth must be 20. filter is 3<em>3</em>20. parameter &#x3D; 3<em>3</em>20 &#x3D; 180. <strong>The connectivity is local in 2D space (e.g. 3*3) but must be full along input depth(channel) which is 20 in the example</strong> notice that in the illustration below, there are 5 neurons in the same depth dimension. these 5 neurons have the same receptive field but they essentially have different filters. so the weights associated with them are different.<br><img src="https://cs231n.github.io/assets/cnn/depthcol.jpeg" alt="Filter illustration"></p>
<h3 id="Spatial-Arrangement"><a href="#Spatial-Arrangement" class="headerlink" title="Spatial Arrangement"></a>Spatial Arrangement</h3><p>the hyper-parameters (parameters that are not trained using gradient descent) are <strong>depth (output-channel size), stride, and padding</strong></p>
<ol>
<li>the depth is the number of filter we use. each filter could be learning different information from the input. for example, in first convolutional layer, neuron that look at the same receptive field could be extracting different information from it. some may take information of oriented edges, some looking color etc. we refer to neuron that look at the same region as <strong>depth column</strong></li>
<li>stride is how much we move the filter each time. if the stride is the same as filter size, there is no overlap between the receptive field that each time the filter is looking at. if stride is smaller than filter size, then there is overlap. </li>
<li>sometimes we pad the input with 0 around all edges. the padding is commonly chosen so we preserve the original height and width of the input.</li>
</ol>
<p>The formula for input size after convolution is  $$\frac{InputWidth-Kernel Width+2Padding}{Stride} + 1$$. </p>
<p>Notice when padding is (kernel size -1)&#x2F;2 , we preserve the original width and height when the stride is 1. </p>
<p>The stride must be designed such that it only give us integer after we divide by it. Else it is invalid.<br><img src="https://cs231n.github.io/assets/cnn/stride.jpeg" alt="Illustration of Stride"></p>
<h3 id="Parameter-Sharing-Assumption"><a href="#Parameter-Sharing-Assumption" class="headerlink" title="Parameter Sharing Assumption"></a>Parameter Sharing Assumption</h3><p>In AlexNet, we know that our 227 * 277 <em>3 input have receptive field of 11, stride of 4 and no padding of zero. Calculate the output, we get that in our first convolutional layer, we should have ((227-11)&#x2F;4) + 1 &#x3D; 55 as output width and height. since we are using 96 kernel, our output volume is 55</em>55<em>96. each of the neuron in this 55</em>55<em>96 output is connected to a 11</em>11<em>3 region in input. So, together we would have 55</em>55<em>96</em>11<em>11</em>3 &#x3D; 105,705,600 parameters. The magnitude of parameter seems to be exploding already in the very first convolutional layer. To reduce the parameters, needed, we introduce the assumption of  <strong>parameter sharing</strong>.  The assumption here is that if one feature is useful to compute at some spatial position, then it should also be useful to compute at a different position. under parameter sharing the 55*55 neuron in each of the 96 depth slices will be using same parameters. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/27/Why%20CNN/" data-id="clqnlekyk00002eih5epr3uid" data-title="Why switch to CNN from MLP" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-AlexNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/21/AlexNet/" class="article-date">
  <time class="dt-published" datetime="2023-12-21T06:48:02.551Z" itemprop="datePublished">2023-12-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/21/AlexNet/">AlexNet Practice</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><p>use <strong>Dropout</strong> rate of 0.5. <strong>ReLU</strong> as activation. 5 convolution layers and 3 fully connected linear layers. the innovation is in the activation function, the overlapping pooling <strong>kernel_size&#x3D;3</strong> and <strong>stride&#x3D;2</strong>. The dropout reduce the overfitting and overlapped pooling increase the accuracy</p>
<h2 id="AlexNet-Code"><a href="#AlexNet-Code" class="headerlink" title="AlexNet Code"></a>AlexNet Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">AlexNet2</span>(d2l.Classifier):</span><br><span class="line"><span class="keyword">def</span>  <span class="title function_">__init__</span>(<span class="params">self, lr=<span class="number">0.01</span>, num_classes=<span class="number">1000</span></span>):</span><br><span class="line">	<span class="built_in">super</span>().__init__()</span><br><span class="line">	self.save_hyperparameters()</span><br><span class="line">	self.net = nn.Sequential(</span><br><span class="line">		nn.Conv2d(<span class="number">3</span>, <span class="number">48</span>*<span class="number">2</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">		nn.Conv2d(<span class="number">48</span>*<span class="number">2</span>, <span class="number">128</span>*<span class="number">2</span>, kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>), nn.ReLU(),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">		nn.Conv2d(<span class="number">128</span>*<span class="number">2</span>, <span class="number">192</span>*<span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">		nn.Conv2d(<span class="number">192</span>*<span class="number">2</span>, <span class="number">192</span>*<span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">		nn.Conv2d(<span class="number">192</span>*<span class="number">2</span>, <span class="number">128</span>*<span class="number">2</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>), nn.ReLU(),</span><br><span class="line">		nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">		nn.Flatten(),</span><br><span class="line">		nn.Linear(<span class="number">6400</span>,<span class="number">4096</span>), nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">		nn.Linear(<span class="number">4096</span>, <span class="number">4096</span>), nn.ReLU(),nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">		nn.Linear(<span class="number">4096</span>, num_classes)</span><br><span class="line">		)</span><br><span class="line">	self.net.apply(d2l.init_cnn)</span><br><span class="line">AlexNet2().layer_summary((<span class="number">1</span>,<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>))</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/21/AlexNet/" data-id="clqeule7n0000lmih6me4174z" data-title="AlexNet Practice" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LeNet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/20/LeNet/" class="article-date">
  <time class="dt-published" datetime="2023-12-21T03:08:02.221Z" itemprop="datePublished">2023-12-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/20/LeNet/">LeNet Practice</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h1><p>supervised learning. at high level, LeNet-5 has convolutional encoder consist of 2 convolutional layers and 3 fully connected layers (MLP of 3 layers). LeNet use <strong>average pooling</strong> and <strong>tanh</strong>  activation function.  we use <strong>Xavier initialization</strong> to make sure the mean and variance of output of fully connected layer is the same as the mean and variance of input. </p>
<h2 id="Initialize-CNN-using-Uniform-Xavier-Initialization"><a href="#Initialize-CNN-using-Uniform-Xavier-Initialization" class="headerlink" title="Initialize CNN using Uniform Xavier Initialization"></a>Initialize CNN using Uniform Xavier Initialization</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span>  <span class="title function_">initialize_cnn</span>(<span class="params">module</span>):</span><br><span class="line"><span class="keyword">if</span>  <span class="built_in">type</span>(module)==nn.Linear <span class="keyword">or</span>  <span class="built_in">type</span>(module)==nn.Conv2d:</span><br><span class="line">	nn.init.xavier_uniform_(module.weight)</span><br></pre></td></tr></table></figure>


<h2 id="Create-LeNet-with-MaxPool-and-ReLU"><a href="#Create-LeNet-with-MaxPool-and-ReLU" class="headerlink" title="Create LeNet with MaxPool and ReLU"></a>Create LeNet with MaxPool and ReLU</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span>  <span class="title class_">LeNet1</span>(d2l.Classifier):</span><br><span class="line">	<span class="keyword">def</span>  <span class="title function_">__init__</span>(<span class="params">self, lr=<span class="number">0.01</span>, num_classes=<span class="number">10</span></span>):</span><br><span class="line">		<span class="built_in">super</span>().__init__()</span><br><span class="line">		self.save_hyperparameters()</span><br><span class="line">		self.net = nn.Sequential(</span><br><span class="line">			nn.LazyConv2d(out_channels=<span class="number">6</span>,</span><br><span class="line">			kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">			nn.ReLU(),</span><br><span class="line">			nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">			nn.LazyConv2d(out_channels=<span class="number">16</span>,kernel_size=<span class="number">5</span>),</span><br><span class="line">			nn.ReLU(),</span><br><span class="line">			nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>),</span><br><span class="line">			nn.Flatten(),</span><br><span class="line">			nn.LazyLinear(<span class="number">120</span>),</span><br><span class="line">			nn.ReLU(),</span><br><span class="line">			nn.LazyLinear(<span class="number">84</span>),</span><br><span class="line">			nn.ReLU(),</span><br><span class="line">			nn.LazyLinear(num_classes))</span><br></pre></td></tr></table></figure>

<h2 id="Print-Layer-Summary"><a href="#Print-Layer-Summary" class="headerlink" title="Print Layer Summary"></a>Print Layer Summary</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span>  <span class="title function_">layer_summary1</span>(<span class="params">self, input_shape</span>):</span><br><span class="line">	X = torch.randn(*input_shape)</span><br><span class="line">	<span class="keyword">for</span> layer <span class="keyword">in</span>  self.net:</span><br><span class="line">	X = layer(X)</span><br><span class="line">	<span class="built_in">print</span>(layer.__class__.__name__,<span class="string">&quot;output shape:\t&quot;</span>, X.shape)</span><br><span class="line">model1 = LeNet1()</span><br><span class="line">model1.layer_summary((<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br></pre></td></tr></table></figure>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">trainer = d2l.Trainer(max_epochs=<span class="number">20</span>, num_gpus=<span class="number">1</span>)</span><br><span class="line">data = d2l.FashionMNIST(batch_size=<span class="number">128</span>)</span><br><span class="line">model = LeNet1(lr=<span class="number">0.1</span>)</span><br><span class="line">model.apply_init([<span class="built_in">next</span>(<span class="built_in">iter</span>(data.get_dataloader(<span class="literal">True</span>)))[<span class="number">0</span>]], initialize_cnn)</span><br><span class="line">trainer.fit(model, data)</span><br></pre></td></tr></table></figure>




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/20/LeNet/" data-id="clqeule7r0001lmih7fpj98af" data-title="LeNet Practice" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-firstBlog" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/16/firstBlog/" class="article-date">
  <time class="dt-published" datetime="2023-12-16T08:19:57.000Z" itemprop="datePublished">2023-12-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/16/firstBlog/">firstBlog</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/16/firstBlog/" data-id="clq7tkvlg00012gihgc34a9wb" data-title="firstBlog" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/16/hello-world/" class="article-date">
  <time class="dt-published" datetime="2023-12-16T08:10:02.387Z" itemprop="datePublished">2023-12-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/16/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/16/hello-world/" data-id="clq7tkvld00002gihbksfbio9" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/02/15/perceptrons/">(no title)</a>
          </li>
        
          <li>
            <a href="/2023/12/29/graycode/">LeetCode 89 Gray Code</a>
          </li>
        
          <li>
            <a href="/2023/12/29/subset%20II/">LeetCode 90 Subset II</a>
          </li>
        
          <li>
            <a href="/2023/12/28/GoogLeNet/">GoogleNet Practice</a>
          </li>
        
          <li>
            <a href="/2023/12/27/Why%20CNN/">Why switch to CNN from MLP</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>